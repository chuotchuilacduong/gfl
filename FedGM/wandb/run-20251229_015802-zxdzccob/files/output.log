G:\conda\envs\gfl\Lib\site-packages\torch_geometric\data\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
round # 0		sampled_clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Traceback (most recent call last):
  File "G:\Code\python\FedGM\main.py", line 32, in <module>
    trainer.train()
  File "G:\Code\python\FedGM\flcore\trainer.py", line 122, in train
    self.clients[client_id].execute()
  File "G:\Code\python\FedGM\flcore\fedlog\client.py", line 132, in execute
    self.train_generator()
  File "G:\Code\python\FedGM\flcore\fedlog\client.py", line 173, in train_generator
    sub_idx, sub_edge_index, _, _ = k_hop_subgraph(i, k_hop, local_graph.edge_index, relabel_nodes=True)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "G:\conda\envs\gfl\Lib\site-packages\torch_geometric\utils\_subgraph.py", line 353, in k_hop_subgraph
    subsets.append(col[edge_mask])
                   ~~~^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
